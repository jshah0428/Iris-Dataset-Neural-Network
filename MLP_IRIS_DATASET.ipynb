{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Pytorch MLP example problem"
      ],
      "metadata": {
        "id": "9-9eu2bTybkM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9cJcSbSbd48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699f6178-d485-47ef-e6d8-6ce0b9b29578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120, 4)\n",
            "Epoch 1/50, Loss: 1.0597\n",
            "Epoch 2/50, Loss: 0.8780\n",
            "Epoch 3/50, Loss: 0.7375\n",
            "Epoch 4/50, Loss: 0.6303\n",
            "Epoch 5/50, Loss: 0.5500\n",
            "Epoch 6/50, Loss: 0.4798\n",
            "Epoch 7/50, Loss: 0.4185\n",
            "Epoch 8/50, Loss: 0.3645\n",
            "Epoch 9/50, Loss: 0.3183\n",
            "Epoch 10/50, Loss: 0.2835\n",
            "Epoch 11/50, Loss: 0.2490\n",
            "Epoch 12/50, Loss: 0.2213\n",
            "Epoch 13/50, Loss: 0.1968\n",
            "Epoch 14/50, Loss: 0.1760\n",
            "Epoch 15/50, Loss: 0.1593\n",
            "Epoch 16/50, Loss: 0.1441\n",
            "Epoch 17/50, Loss: 0.1319\n",
            "Epoch 18/50, Loss: 0.1209\n",
            "Epoch 19/50, Loss: 0.1128\n",
            "Epoch 20/50, Loss: 0.1080\n",
            "Epoch 21/50, Loss: 0.0997\n",
            "Epoch 22/50, Loss: 0.0943\n",
            "Epoch 23/50, Loss: 0.0900\n",
            "Epoch 24/50, Loss: 0.0870\n",
            "Epoch 25/50, Loss: 0.0865\n",
            "Epoch 26/50, Loss: 0.0838\n",
            "Epoch 27/50, Loss: 0.0767\n",
            "Epoch 28/50, Loss: 0.0763\n",
            "Epoch 29/50, Loss: 0.0721\n",
            "Epoch 30/50, Loss: 0.0724\n",
            "Epoch 31/50, Loss: 0.0697\n",
            "Epoch 32/50, Loss: 0.0660\n",
            "Epoch 33/50, Loss: 0.0658\n",
            "Epoch 34/50, Loss: 0.0645\n",
            "Epoch 35/50, Loss: 0.0625\n",
            "Epoch 36/50, Loss: 0.0639\n",
            "Epoch 37/50, Loss: 0.0622\n",
            "Epoch 38/50, Loss: 0.0611\n",
            "Epoch 39/50, Loss: 0.0597\n",
            "Epoch 40/50, Loss: 0.0588\n",
            "Epoch 41/50, Loss: 0.0574\n",
            "Epoch 42/50, Loss: 0.0568\n",
            "Epoch 43/50, Loss: 0.0601\n",
            "Epoch 44/50, Loss: 0.0620\n",
            "Epoch 45/50, Loss: 0.0557\n",
            "Epoch 46/50, Loss: 0.0549\n",
            "Epoch 47/50, Loss: 0.0549\n",
            "Epoch 48/50, Loss: 0.0540\n",
            "Epoch 49/50, Loss: 0.0527\n",
            "Epoch 50/50, Loss: 0.0560\n",
            "Test Accuracy: 100.00%\n",
            "Predicted class: versicolor\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "x = iris.data #outputs 4 rows - sepal length, sepal width, petal length, nd petal width.\n",
        "y = iris.target #has the respective flower that is labeled - 0 = iris setoda, 1 = iris versicolor, 2 = virginica\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train) #this is critical in making sure all features are on same scale(mean of 0, variance 1)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32) #this transforms all of the variables in to pytorch tensors for usability.\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "'''\n",
        "- batching =  Basically breaks it down, and runs training in small portions of the entire dataset.,\n",
        "- shuffling = making the model see the dataset differently each epoch - helps better generalize the model.\n",
        "- while the dataset is smaller, and this isn't strictly necessary, we do this to make sure we implement batching and shuffling through the epochs.\n",
        "'''\n",
        "class IrisDataset(Dataset):\n",
        "  def __init__(self, data, labels):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "train_dataset = IrisDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = IrisDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 10, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 10, shuffle = False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class mlp(nn.Module):\n",
        "  def __init__(self, input_size, hidden_sizes, output_size): #number of input features, number of neurons in each hidden layer(typically passed as a list), number of output features,\n",
        "    super(mlp, self).__init__() # this helps initialize the base class(nn.Module - this is class for all NN in pytorch). without it, the mlp class will not be accessible.\n",
        "    '''\n",
        "      summation of input features multiplied by first layer weights, added to a bias, and then sent into reLU activation function.\n",
        "      relu function added to introduce nonlinearity. can u ad\n",
        "    '''\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_sizes[0]), #input is the first layer, output is the first HIDDEN layer\n",
        "        nn.ReLU(), #used because it is the most generalized activation function.\n",
        "        nn.Linear(hidden_sizes[0], hidden_sizes[1]), #goes from first to second hidden layer\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_sizes[1], output_size)#goes from second hidden layer to output layer.\n",
        "\n",
        "    )\n",
        "\n",
        "  '''while redundant, and already defined in the nn.Module super class, it is good practice to write a forward propogation method'''\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "# --- instantiating the model ---\n",
        "input_size= X_train.shape[1] #number of columns = number of input features\n",
        "hidden_sizes = [64,32] #number of neurons in the first and second hidden layer respectively\n",
        "output_size = 3 #we know only three outputs exist, setosa, versicolod, and virginica.\n",
        "\n",
        "model = mlp(input_size, hidden_sizes, output_size)\n",
        "\n",
        "\n",
        "# --- define loss function(cross entropy because it is a classification problem) and optimizer ---\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "''' model.parameter = parameters including weights and biases. lr = learning rate. small learning rate is slower computation, but more stable convergence. makes sure you don't miss the optimal weights and biases).'''\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) #most general and widely used optimizer function. slides cover more optimizer functions.\n",
        "\n",
        "\n",
        "# --- model training ---\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  running_loss=0.0 #used to measure how well the model is performing.\n",
        "\n",
        "\n",
        "  '''\n",
        "  following code describes the process of forward pass, loss calculation, back-propogation, and optimization in a multi-layer perceptron.\n",
        "  '''\n",
        "  for inputs, labels in train_loader: #this demonstrates the batching. it performs the entire learning process for all the smaller batches in the epoch.\n",
        "    optimizer.zero_grad() #zero's out the gradients so that we don't have any recollection of the gradients from precious batches.\n",
        "    outputs = model(inputs) #forward propogation - internally calls the forward method.\n",
        "    loss = loss_function(outputs, labels) #calculates the loss function\n",
        "    loss.backward() #back prop\n",
        "    optimizer.step() #calculates optimizer.\n",
        "\n",
        "    running_loss+=loss.item() #helps keep track of the average loss over the entire epoch.\n",
        "\n",
        "  print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "  # --- model evalutaion ---\n",
        "\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad(): #disables gradient calculation, which is really only important to save memory and increase computation speed.\n",
        "    for inputs, labels in test_loader:\n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs, 1) #we do .max because outputs has the confidence of ALL the classes. we only want what is the most likely predicted, which is the index(hence 1).\n",
        "      total+= labels.size(0)\n",
        "      # print(predicted, \"---predicted\")\n",
        "      # print(labels, \"---- labels\")\n",
        "      correct+= (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "accuracy = correct/total\n",
        "print(f\"Test Accuracy: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predictions"
      ],
      "metadata": {
        "id": "9J0KhWd8g8g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input_data):\n",
        "\n",
        "    input_data = scaler.transform([input_data])\n",
        "    input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        #this gets the output\n",
        "        outputs = model(input_tensor)\n",
        "        _, predicted = torch.max(outputs, 1) #gets the index of the highest predicted value, which is the class.\n",
        "\n",
        "    print(outputs)\n",
        "    print(predicted)\n",
        "\n",
        "    # gets the class name assocciated with the predicted class.\n",
        "    class_names = iris.target_names\n",
        "    predicted_class = class_names[predicted.item()]\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "#predictions\n",
        "sample_inputs = [[5.3, 2.3, 3.3, 1], [6.3, 1.3, 4.3, 2], [4.3, 1.3, 2.3, 3], [2.3, 0.3, 4.3, 2.3]]\n",
        "for sample_input in sample_inputs:\n",
        "  predicted_class = predict(sample_input)\n",
        "  print(f\"Predicted class: {predicted_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5VjAx7Wg9WH",
        "outputId": "4261a8fb-807b-45b7-f67b-a227ba74c4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.8686,  3.9124, -3.5819]])\n",
            "tensor([1])\n",
            "Predicted class: versicolor\n",
            "tensor([[-11.8294,  -0.6093,   3.7085]])\n",
            "tensor([2])\n",
            "Predicted class: virginica\n",
            "tensor([[-10.6116,  -4.2005,   6.1600]])\n",
            "tensor([2])\n",
            "Predicted class: virginica\n",
            "tensor([[-14.0184,  -6.8266,   9.1874]])\n",
            "tensor([2])\n",
            "Predicted class: virginica\n"
          ]
        }
      ]
    }
  ]
}